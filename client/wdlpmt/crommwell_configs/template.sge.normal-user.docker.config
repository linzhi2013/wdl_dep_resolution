include required(classpath("application"))

webservice {
  port = 8000
  interface = master1120
  binding-timeout = 5s
}

backend {
  default = SGE_Docker

  providers {
    SGE_Docker {
      actor-factory = "cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory"
      config {

        # Limits the number of concurrent jobs
        concurrent-job-limit = 500

        # If an 'exit-code-timeout-seconds' value is specified:
        # - check-alive will be run at this interval for every job
        # - if a job is found to be not alive, and no RC file appears after this interval
        # - Then it will be marked as Failed.
        # Warning: If set, Cromwell will run 'check-alive' for every job at this interval

        # exit-code-timeout-seconds = 120

        # `script-epilogue` configures a shell command to run after the execution of every command block.
        #
        # If this value is not set explicitly, the default value is `sync`, equivalent to:
        # script-epilogue = "sync"
        #
        # To turn off the default `sync` behavior set this value to an empty string:
        # script-epilogue = ""

        script-epilogue = "sync && sleep 8 "

        runtime-attributes = """
        String docker
        String root
        Int? cpu = 1
        Int? memory_gb = 2
        String? sge_queue
        """

        # This is very important to make sure we can use the File type variable
        # in SGE enviroments.
        # The value of dockerRoot must be the folder where you run your WDL pipeline,
        # appending the suffix '/cromwell-executions',
        # e.g.
        # dockerRoot = "/export/personal/menggl/my_test/Hi-C_Assembly_pipeline/test/LibraryQC_FileType/cromwell-executions"
        #
        # We replace the '__DOCKER_ROOT__' to the real path whenever and whereever we run our WDL pipeline each time:
        dockerRoot = __DOCKER_ROOT__

        # You must ensure each SGE-node can access to the Docker images being used.
        # To this end, you may deploy all related Docker images to each batchly,
        # for example, using command: ansible node -m shell -a "YouCommand"
        # Or place those images in a Docker repository.
        submit-docker  = """
        qsub \
        -terse \
        -V \
        -b y \
        -N ${job_name} \
        -wd ${cwd} \
        -o ${out}.qsub \
        -e ${err}.qsub \
        -l vf=${memory_gb}G \
        ${"-pe smp " + cpu} \
        ${"-q " + sge_queue} \
        docker run --rm  --user $(id -u):$(id -g) -a STDERR -v ${cwd}:${docker_cwd} -v ${root}:${root} ${docker}  /bin/sh ${docker_script}
        """

        kill = "qdel ${job_id}"
        check-alive = "qstat -j ${job_id}"
        job-id-regex = "(\\d+)"

        kill-docker = "qdel ${job_id}"
        check-alive-docker = "qstat -j ${job_id}"
        job-id-regex-docker = "(\\d+)"
      }
    }
}
}
docker.hash-lookup.enabled = false

